{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of all packages used in this notebook\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pycountry\n",
    "from pycountry_convert.convert_country_alpha2_to_continent_code import country_alpha2_to_continent_code\n",
    "from pycountry_convert.convert_continent_code_to_continent_name import convert_continent_code_to_continent_name\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, PowerTransformer, QuantileTransformer \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from column_names import get_column_names\n",
    "from utils.data_handling import DataHandling\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r \"requirements.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip dataset, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cost-of-living_v2.csv'\n",
    "\n",
    "# check if file already exists\n",
    "if os.path.exists(filename):\n",
    "    print('File {} exists.'.format(filename))\n",
    "\n",
    "else:\n",
    "    \n",
    "    zip_file = 'global-cost-of-living.zip'\n",
    "    \n",
    "    # check if kaggle zip-file already exists\n",
    "    if os.path.exists(zip_file):\n",
    "        print('File {} exists.'.format(zip_file))\n",
    "    \n",
    "    else:\n",
    "        # Download files from kaggle\n",
    "        ! kaggle datasets download -d mvieira101/global-cost-of-living\n",
    "        \n",
    "\n",
    "    # end if\n",
    "\n",
    "    # Unpacking files\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('')\n",
    "    print('Unpacking {}.'.format(zip_file))\n",
    "\n",
    "# end if"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# give the columns informative names\n",
    "df.columns = get_column_names()\n",
    "\n",
    "global_random_state = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains cities in several countries. For each city there are several costs, e.g. for groceries and beverages, transportation, leisure time, clothing, housing.\n",
    "Additionally it contains information about salaries and mortgages.\n",
    "The last columns is called `data_quality` and contains a flag. It is 0 if Numbeo considers that more contributors are needed to increase data quality and 1 elsewise.\n",
    "\n",
    "Let's see how many rows have sufficient data and drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dtype of column data_quality\n",
    "col_quality = 'data_quality'\n",
    "print('Column {} has type {}.'.format(\n",
    "    col_quality,\n",
    "    df[col_quality].dtype\n",
    "))\n",
    "\n",
    "# convert dtype of data quality column to bool\n",
    "df[col_quality] = pd.Series(df[col_quality], dtype=bool)\n",
    "\n",
    "# count rows with good data quality\n",
    "print('{} of {} cities have good data quality!'.format(\n",
    "    df[col_quality].sum(),\n",
    "    df.shape[0]\n",
    "))\n",
    "\n",
    "# drop all rows with bad data quality\n",
    "df_quality = df.loc[df[col_quality],:]\n",
    "df_quality = df_quality.drop(labels=col_quality, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = (df_quality.shape[0]-df_quality.count()).sort_values(ascending=False)\n",
    "\n",
    "print('{} of {} columns do not have any missing values\\n'.format(\n",
    "    df_missing.value_counts()[0],\n",
    "    df_missing.shape[0]\n",
    "))\n",
    "\n",
    "print('Missing values by columns:\\n{}'.format(\n",
    "    df_missing\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a nan checker we will need later\n",
    "def na_check(df, cols):\n",
    "    \"\"\"\n",
    "    Checks for na values in the columns cols of Dataframe df.\n",
    "    Outputs True if any of the columns has a na value and False elsewise.\n",
    "    \"\"\"\n",
    "    contains_na = df.loc[:,cols].isna().any(axis=1).any()\n",
    "    return contains_na\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality.select_dtypes(include='object').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that all columns except `city` and `country` are numerical.\n",
    "\n",
    "Let's take a look at the distributions of all columns next. First we will have a look at the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_hist(categorical, limit=0):\n",
    "    \"\"\"\n",
    "    Plot a histogram for categoricals with 90Â° ticks.\n",
    "    Limit output to the highest 'limit' counts.\n",
    "    \"\"\"\n",
    "    categorical_counts = categorical.value_counts()\n",
    "\n",
    "    limit = limit if limit != 0 else categorical_counts.shape[0]\n",
    "\n",
    "    categorical_dict = categorical_counts.iloc[:limit].to_dict()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.bar(categorical_dict.keys(), categorical_dict.values())\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "    return ax\n",
    "\n",
    "n_countries = 20\n",
    "\n",
    "ax = plot_categorical_hist(df_quality['country'], limit=n_countries)\n",
    "ax.set_title(f'Country distribution of the {n_countries} most frequent countries.')\n",
    "ax.set_xlabel('Country')\n",
    "ax.set_ylabel('# of cities')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like cities in the US and European cities are highly present in this dataset.\n",
    "\n",
    "This raises the question, which continents the cities are located on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_country_to_continent(country_name):\n",
    "    \"\"\" \n",
    "    Function provides continent name for a country name.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize continent_name and country_obj\n",
    "    continent_name = 'Unknown'\n",
    "    country_obj = []\n",
    "\n",
    "    # get the pycountry.Country object\n",
    "    try:\n",
    "        country_obj = pycountry.countries.lookup(country_name)\n",
    "    \n",
    "    except LookupError:\n",
    "        # Print info\n",
    "        print('Could not find {} with lookup function. Trying search_fuzzy.'.format(\n",
    "            country_name\n",
    "        ))\n",
    "\n",
    "        # try search fuzzy instead\n",
    "        try:\n",
    "            country_list = pycountry.countries.search_fuzzy(country_name)           \n",
    "\n",
    "            # print information whether multiple results occured\n",
    "            if len(country_list) > 1:\n",
    "                print('Expected only one country during search_fuzzy, but got {}!.'.format(\n",
    "                    len(country_list)\n",
    "                ))\n",
    "                \n",
    "            else:\n",
    "                print('search_fuzzy was successful with exactly one result!')    \n",
    "            # end if\n",
    "\n",
    "            country_obj = country_list[0]\n",
    "            print('Using \"{}\" for \"{}\".'.format(\n",
    "                country_obj.name,\n",
    "                country_name\n",
    "            ))\n",
    "\n",
    "        except:\n",
    "            print('{} not found. Country will have no continent'.format(\n",
    "                country_name\n",
    "            ))\n",
    "            \n",
    "        # end try\n",
    "    # end try\n",
    "        \n",
    "    if str(type(country_obj)) == \"<class 'pycountry.db.Country'>\": # isinstance does not work\n",
    "        # convert alpha_2 value of country object into continent name         \n",
    "        country_code = country_obj.alpha_2\n",
    "        continent_code = country_alpha2_to_continent_code(country_code)\n",
    "        continent_name = convert_continent_code_to_continent_name(continent_code)\n",
    "    # end if\n",
    "\n",
    "    return continent_name\n",
    "\n",
    "# add a continen column to the dataset\n",
    "df_quality['continent'] = df_quality['country'].apply(convert_country_to_continent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_categorical_hist(df_quality['continent'])\n",
    "ax.set_title('Continent distribution')\n",
    "ax.set_xlabel('Continent')\n",
    "ax.set_ylabel('# of cities')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our first impression: Most of the cities are in Europe and Nothern America. Unknown refers to countries, where the continent could not be automatically assigned.\n",
    "\n",
    "Let's look at the other distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_quality.select_dtypes(include='float')\n",
    "n_plots = df_hist.shape[1]\n",
    "n_cols = 6\n",
    "n_rows = int(np.ceil( n_plots / n_cols ))\n",
    "fig, ax  = plt.subplots(n_rows, n_cols, squeeze=True, figsize=(3*n_cols, 3*n_rows))\n",
    "ax.resize((ax.size,))\n",
    "\n",
    "for i in range(n_plots):\n",
    "    df_hist.iloc[:,i].hist(ax=ax[i])\n",
    "    ax[i].set_title(df_hist.columns[i])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where the rental prices for apartments are the highest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'country',\n",
    "    'apartment_rent_1_room_centre', \n",
    "    'apartment_rent_1_room_outside', \n",
    "    'apartment_rent_3_room_centre', \n",
    "    'apartment_rent_3_room_outside', \n",
    "    'apartment_price_centre',\n",
    "    'apartment_price_outside',\n",
    "]\n",
    "print('Are there na values in the analyzed columns? {}'.format(\n",
    "    na_check(df_quality, cols)\n",
    "))\n",
    "\n",
    "# drop the rows with na values\n",
    "df_apartment_prices = df_quality.loc[:, cols].dropna(axis=0, how='any')\n",
    "\n",
    "# mean apartment prices and rent grouped by country\n",
    "mean_apartment_prices = df_apartment_prices.groupby('country').mean()\n",
    "mean_apartment_prices.head()\n",
    "\n",
    "# Which countries are in the Top 15 for all categories\n",
    "n_largest = 15\n",
    "countries_high_prices = set(mean_apartment_prices.index)\n",
    "for col in cols[1:]:\n",
    "    countries_high_prices = countries_high_prices & set(mean_apartment_prices.nlargest(n_largest, col).index)\n",
    "# end for\n",
    "print(countries_high_prices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 6 countries are amoung the most expensive apartments in all categories!\n",
    "\n",
    "Let's see where people pay the highest share of their salary for rent. \n",
    "For this, we will calculate the mean of the rents for 1- and 3-room-apartments in- and outside of the city centres.\n",
    "This is suggested by the [numbeo methodology](https://www.numbeo.com/common/motivation_and_methodology.jsp).\n",
    "The mean is divided by the salary.\n",
    "\n",
    "Since the mean and division are not legitimate with missing values, we drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'apartment_rent_1_room_centre', \n",
    "    'apartment_rent_1_room_outside', \n",
    "    'apartment_rent_3_room_centre', \n",
    "    'apartment_rent_3_room_outside', \n",
    "    'salary'\n",
    "]\n",
    "\n",
    "print('Are there na values in the analyzed columns? {}'.format(\n",
    "    na_check(df_quality, cols)\n",
    "))\n",
    "\n",
    "df_quality = df_quality.dropna(axis=0, how='any', subset=cols)\n",
    "\n",
    "df_quality.loc[:,'rent_salary_share'] = df_quality.loc[:,cols].mean(axis=1)/df_quality.loc[:,'salary']\n",
    "\n",
    "# mean over countries\n",
    "mean_rent_salary_share = df_quality.loc[:,['country', 'rent_salary_share', 'salary']].groupby('country').mean()\n",
    "\n",
    "mean_rent_salary_share.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "plt.scatter(mean_rent_salary_share['salary'], mean_rent_salary_share['rent_salary_share'])\n",
    "ax.set_ylim((0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1eb35e6531bd04b4b837975573005e099e8ade0fd8b9ddc6a1acbd27b46e7bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
