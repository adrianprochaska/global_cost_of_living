{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check requirements.txt ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r \"requirements.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of all packages used in this notebook\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pycountry\n",
    "from pycountry_convert.convert_country_alpha2_to_continent_code \\\n",
    "    import country_alpha2_to_continent_code\n",
    "from pycountry_convert.convert_continent_code_to_continent_name \\\n",
    "    import convert_continent_code_to_continent_name\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from column_names import get_column_names\n",
    "from utils.categoricalTransformer import CategoricalTransformer\n",
    "from utils.numericalTransformer import NumericalTransformer\n",
    "\n",
    "from scipy.stats.mstats import pearsonr\n",
    "\n",
    "\n",
    "# Magic commands\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip dataset, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'cost-of-living_v2.csv'\n",
    "\n",
    "# check if file already exists\n",
    "if os.path.exists(filename):\n",
    "    print('File {} exists.'.format(filename))\n",
    "\n",
    "else:\n",
    "\n",
    "    zip_file = 'global-cost-of-living.zip'\n",
    "\n",
    "    # check if kaggle zip-file already exists\n",
    "    if os.path.exists(zip_file):\n",
    "        print('File {} exists.'.format(zip_file))\n",
    "    else:\n",
    "        # Download files from kaggle\n",
    "        ! kaggle datasets download -d \"mvieira101/global-cost-of-living\"\n",
    "    # end if\n",
    "\n",
    "    # Unpacking files\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall('')\n",
    "    print('Unpacking {}.'.format(zip_file))\n",
    "\n",
    "# end if"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# give the columns informative names\n",
    "df.columns = get_column_names()\n",
    "\n",
    "global_random_state = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create fig-folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('fig'):\n",
    "    os.mkdir('fig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains cities in several countries. For each city there are several costs, e.g. for groceries and beverages, transportation, leisure time, clothing, housing.\n",
    "Additionally it contains information about salaries and mortgages.\n",
    "The last columns is called `data_quality` and contains a flag. It is 0 if Numbeo considers that more contributors are needed to increase data quality and 1 elsewise.\n",
    "\n",
    "Let's see how many rows have sufficient data and drop the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dtype of column data_quality\n",
    "col_quality = 'data_quality'\n",
    "print('Column {} has type {}.'.format(\n",
    "    col_quality,\n",
    "    df[col_quality].dtype\n",
    "))\n",
    "\n",
    "# convert dtype of data quality column to bool\n",
    "df[col_quality] = pd.Series(df[col_quality], dtype=bool)\n",
    "\n",
    "# count rows with good data quality\n",
    "print('{} of {} cities have good data quality!'.format(\n",
    "    df[col_quality].sum(),\n",
    "    df.shape[0]\n",
    "))\n",
    "\n",
    "# drop all rows with bad data quality\n",
    "df_quality = df.loc[df[col_quality], :]\n",
    "df_quality = df_quality.drop(labels=col_quality, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = (df_quality.shape[0]-df_quality.count())\n",
    "df_missing = df_missing.sort_values(ascending=False)\n",
    "\n",
    "print('{} of {} columns do not have any missing values\\n'.format(\n",
    "    df_missing.value_counts()[0],\n",
    "    df_missing.shape[0]\n",
    "))\n",
    "\n",
    "print('Missing values by columns:\\n{}'.format(\n",
    "    df_missing\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a nan checker we will need later\n",
    "def na_check(df, cols):\n",
    "    \"\"\"\n",
    "    Checks for na values in the columns cols of Dataframe df.\n",
    "    Outputs True if any of the columns has a na value and False elsewise.\n",
    "    \"\"\"\n",
    "    contains_na = df.loc[:, cols].isna().any(axis=1).any()\n",
    "    return contains_na"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the column datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality.select_dtypes(include='object').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that all columns except `city` and `country` are numerical.\n",
    "\n",
    "Let's take a look at the distributions of all columns next. First we will have a look at the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_hist(categorical, limit=0):\n",
    "    \"\"\"\n",
    "    Plot a histogram for categoricals with 90Â° ticks.\n",
    "    Limit output to the highest 'limit' counts.\n",
    "    \"\"\"\n",
    "    categorical_counts = categorical.value_counts()\n",
    "\n",
    "    limit = limit if limit != 0 else categorical_counts.shape[0]\n",
    "\n",
    "    categorical_dict = categorical_counts.iloc[:limit].to_dict()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    plt.bar(categorical_dict.keys(), categorical_dict.values())\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "    return ax\n",
    "# end def\n",
    "\n",
    "\n",
    "n_countries = 20\n",
    "str_title = (f'Country distribution of the {n_countries}'\n",
    "             'most frequent countries.')\n",
    "\n",
    "ax = plot_categorical_hist(df_quality['country'], limit=n_countries)\n",
    "ax.set_title(str_title)\n",
    "ax.set_xlabel('Country')\n",
    "ax.set_ylabel('# of cities')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like cities in the US and European cities are highly present in this dataset.\n",
    "\n",
    "This raises the question, which continents the cities are located on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_country_to_continent(country_name):\n",
    "    \"\"\"\n",
    "    Function provides continent name for a country name.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize continent_name and country_obj\n",
    "    continent_name = 'Unknown'\n",
    "    country_obj = []\n",
    "\n",
    "    # manual fix for 2 countries\n",
    "    if country_name == 'Ivory Coast':\n",
    "        country_name = 'CÃ´te d\\'Ivoire'\n",
    "\n",
    "    elif country_name == 'Kosovo (Disputed Territory)':\n",
    "        country_name = 'Kosovo'\n",
    "\n",
    "    # end if\n",
    "\n",
    "    # get the pycountry.Country object\n",
    "    try:\n",
    "        country_obj = pycountry.countries.lookup(country_name)\n",
    "\n",
    "    except LookupError:\n",
    "        # Print info\n",
    "        print(('Could not find {} with lookup function. '\n",
    "               'Trying search_fuzzy.').format(\n",
    "            country_name\n",
    "        ))\n",
    "\n",
    "        # try search fuzzy instead\n",
    "        try:\n",
    "            country_list = pycountry.countries.search_fuzzy(country_name)\n",
    "\n",
    "            # print information whether multiple results occured\n",
    "            if len(country_list) > 1:\n",
    "                print(('Expected only one country during '\n",
    "                       'search_fuzzy, but got {}!.').format(\n",
    "                    len(country_list)\n",
    "                ))\n",
    "\n",
    "            else:\n",
    "                print('search_fuzzy was successful with exactly one result!')\n",
    "            # end if\n",
    "\n",
    "            country_obj = country_list[0]\n",
    "            print('Using \"{}\" for \"{}\".'.format(\n",
    "                country_obj.name,\n",
    "                country_name\n",
    "            ))\n",
    "\n",
    "        except LookupError:\n",
    "            print('{} not found. Country will have no continent'.format(\n",
    "                country_name\n",
    "            ))\n",
    "\n",
    "        # end try\n",
    "    # end try\n",
    "\n",
    "    if str(type(country_obj)) == \\\n",
    "       \"<class 'pycountry.db.Country'>\":  # isinstance does not work\n",
    "        # convert alpha_2 value of country object into continent name\n",
    "        country_code = country_obj.alpha_2\n",
    "        continent_code = country_alpha2_to_continent_code(country_code)\n",
    "        continent_name = convert_continent_code_to_continent_name(\n",
    "                continent_code\n",
    "            )\n",
    "    # end if\n",
    "\n",
    "    return continent_name\n",
    "# end def\n",
    "\n",
    "\n",
    "# add a continen column to the dataset\n",
    "df_quality['continent'] = df_quality['country'].apply(\n",
    "        convert_country_to_continent\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_categorical_hist(df_quality['continent'])\n",
    "ax.set_title('Continent distribution')\n",
    "ax.set_xlabel('Continent')\n",
    "ax.set_ylabel('# of cities')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our first impression: Most of the cities are in Europe and Nothern America. Unknown refers to countries, where the continent could not be automatically assigned.\n",
    "\n",
    "Let's look at the other distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_quality.select_dtypes(include='float')\n",
    "n_plots = df_hist.shape[1]\n",
    "n_cols = 6\n",
    "n_rows = int(np.ceil(n_plots / n_cols))\n",
    "fig, ax = plt.subplots(\n",
    "    n_rows,\n",
    "    n_cols,\n",
    "    squeeze=True,\n",
    "    figsize=(3*n_cols, 3*n_rows)\n",
    ")\n",
    "ax.resize((ax.size,))\n",
    "\n",
    "for i in range(n_plots):\n",
    "    df_hist.iloc[:, i].hist(ax=ax[i])\n",
    "    ax[i].set_title(df_hist.columns[i])\n",
    "# end for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also draw a heatmap of the correlation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation heatmap\n",
    "plt.figure(figsize=(17, 17))\n",
    "sns.heatmap(\n",
    "    df_hist.corr(numeric_only=True),\n",
    "    annot=False,\n",
    "    cmap='BrBG',\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how highly correlated the numerical columns in the dataset are.\n",
    "\n",
    "We can see that appartment rents are highly correlated. \n",
    "This means: the higher the rents for 1-room apartments in the city centre are, the higher the rents outside city centre and for 3-room apartments.\n",
    "Interestingly the buying prices do not show the same high correlation with rents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where the rental prices for apartments are the highest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'country',\n",
    "    'apartment_rent_1_room_centre',\n",
    "    'apartment_rent_1_room_outside',\n",
    "    'apartment_rent_3_room_centre',\n",
    "    'apartment_rent_3_room_outside',\n",
    "    'apartment_price_centre',\n",
    "    'apartment_price_outside',\n",
    "]\n",
    "print('Are there na values in the analyzed columns? {}'.format(\n",
    "    na_check(df_quality, cols)\n",
    "))\n",
    "\n",
    "# drop the rows with na values\n",
    "df_apartment_prices = df_quality.loc[:, cols].dropna(axis=0, how='any')\n",
    "\n",
    "# mean apartment prices and rent grouped by country\n",
    "mean_apartment_prices = df_apartment_prices.groupby('country').mean()\n",
    "mean_apartment_prices.head()\n",
    "\n",
    "# Which countries are in the Top 15 for all categories\n",
    "n_largest = 15\n",
    "countries_high_prices = set(mean_apartment_prices.index)\n",
    "for col in cols[1:]:\n",
    "    countries_high_prices = (\n",
    "        countries_high_prices &\n",
    "        set(mean_apartment_prices.nlargest(n_largest, col).index)\n",
    "    )\n",
    "# end for\n",
    "print(sorted(countries_high_prices))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 6 countries are amoung the most expensive apartments in all categories!\n",
    "\n",
    "Let's do some plots to compare the countries with the rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### create and save histogram ###\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# choose and print out column\n",
    "hist_col = cols[4]\n",
    "print(hist_col)\n",
    "\n",
    "# define save name and legend labels\n",
    "save_name = 'fig\\\\hist_{}'.format(hist_col)\n",
    "label_1 = 'all countries'\n",
    "label_2 = 'GHJLSS'\n",
    "\n",
    "# get data for histogram 1\n",
    "y_1 = mean_apartment_prices.loc[:, hist_col]\n",
    "\n",
    "# get data for histogram 2\n",
    "idx_high_prices = mean_apartment_prices.index.isin(countries_high_prices)\n",
    "y_2 = mean_apartment_prices.loc[idx_high_prices, hist_col]\n",
    "\n",
    "# plot histogram 1\n",
    "hist_1 = plt.hist(\n",
    "    y_1,\n",
    "    bins=25,\n",
    "    label=label_1\n",
    ")\n",
    "\n",
    "# plot histogram 2\n",
    "hist_2 = plt.hist(\n",
    "    y_2,\n",
    "    bins=hist_1[1],\n",
    "    color='red',\n",
    "    label=label_2\n",
    ")\n",
    "\n",
    "# set title and axes labels\n",
    "ax.set_title('Average rents of 3-room-apartments\\noutside city centre')\n",
    "ax.set_xlabel('# of countries')\n",
    "ax.set_ylabel('Average rent in $')\n",
    "\n",
    "# create legend\n",
    "ax.legend()\n",
    "\n",
    "# update and save figure\n",
    "plt.savefig(save_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### create and save scatter plot ###\n",
    "\n",
    "# get the relevant columns\n",
    "scatter_data = df_quality.loc[:, ['salary', hist_col]]\n",
    "\n",
    "# define save name and legend labels\n",
    "save_name = 'fig\\\\scatter_{}'.format(hist_col)\n",
    "label_1 = 'all cities'\n",
    "label_2 = 'cities in GHJLSS'\n",
    "\n",
    "# get data for scatter plot 1\n",
    "x_1 = scatter_data.iloc[:, 0]\n",
    "y_1 = scatter_data.iloc[:, 1]\n",
    "\n",
    "# get data for scatter plot 2\n",
    "idx_high_prices = df_quality.loc[:, 'country'].isin(countries_high_prices)\n",
    "x_2 = scatter_data.loc[idx_high_prices].iloc[:, 0]\n",
    "y_2 = scatter_data.loc[idx_high_prices].iloc[:, 1]\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# scatter plot 1\n",
    "ax.scatter(\n",
    "    x_1,\n",
    "    y_1,\n",
    "    label=label_1\n",
    ")\n",
    "\n",
    "# scatter plot 2\n",
    "ax.scatter(\n",
    "    x_2,\n",
    "    y_2,\n",
    "    c=['#ff0000'],\n",
    "    label=label_2\n",
    ")\n",
    "\n",
    "# set title and axes labels\n",
    "ax.set_title(('Average rents of 3-room-apartments outside city centre'\n",
    "              '\\nover average salary in different cities'))\n",
    "ax.set_xlabel('Average salary')\n",
    "ax.set_ylabel('Average rent in $')\n",
    "\n",
    "# create legend\n",
    "ax.legend()\n",
    "\n",
    "# update and save figure\n",
    "plt.savefig(save_name)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a strong linear correlation between salary and the rental prices of 3-room-apartments outside city centres. The 6 countries have medium to very high salaries. Interestingly the highest salary in the data set is included in the 6 countries.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to create a model to precict the salary based on the other features.\n",
    "For this we need to clean the data.\n",
    "\n",
    "Later we want to execute a grid search on some influence factors, which is why we will implement it in a pipeline.\n",
    "\n",
    "For the na values in the data and handling categorical values, two new transformers where created.\n",
    "Those are used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate pipeline\n",
    "pipe = Pipeline([\n",
    "    # 1 preprocessing\n",
    "    ('preprocessing', FeatureUnion([\n",
    "        # .1 handle categoricals\n",
    "        ('categorical_pipe', Pipeline([\n",
    "            # .1 drop continent columns or not\n",
    "            ('drop_cat_cols', CategoricalTransformer()),\n",
    "            # .2 one-hot-encoding\n",
    "            ('one_hot', OneHotEncoder(sparse_output=False,\n",
    "                                      handle_unknown='infrequent_if_exist'))\n",
    "        ])),\n",
    "        # .2 handle numericals\n",
    "        ('numerical_pipe', Pipeline([\n",
    "            # .1 handle na values\n",
    "            ('num_na_handling', NumericalTransformer()),\n",
    "            # .2 polynomial features only for numericals\n",
    "            ('polynomial', PolynomialFeatures())\n",
    "        ]))\n",
    "    ])),\n",
    "    # 2 choose a standardizer\n",
    "    ('scaler', StandardScaler()),  # might be changed in the grid search\n",
    "    # 3 model kind\n",
    "    ('model', LassoCV())  # might be changed in the grid search\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the pipeline parameters\n",
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters for grid search\n",
    "params = {\n",
    "    # # min frequency of categories for one-hot-encoding\n",
    "    # 'preprocessing__categorical_pipe__one_hot__min_frequency': [10],\n",
    "    # max number of categories for one-hot-encoding  # list(range(85, 115)),\n",
    "    'preprocessing__categorical_pipe__one_hot__max_categories':\n",
    "        list(range(90, 100)),\n",
    "    # drop continent column\n",
    "    'preprocessing__categorical_pipe__drop_cat_cols__b_drop_continent':\n",
    "        [False],\n",
    "    # impute method,\n",
    "    'preprocessing__numerical_pipe__num_na_handling__impute_method':\n",
    "        ['mean'],  # 'median',\n",
    "    # max_na_share\n",
    "    'preprocessing__numerical_pipe__num_na_handling__max_na_share':\n",
    "        [1],\n",
    "    # polynomial degree\n",
    "    'preprocessing__numerical_pipe__polynomial__degree':\n",
    "        [1],\n",
    "    # scaler\n",
    "    'scaler':\n",
    "        [StandardScaler()],\n",
    "        # , Normalizer(), QuantileTransformer(), PowerTransformer()],\n",
    "    # model\n",
    "    'model':\n",
    "        [LassoCV(tol=1e-2)],  # , LinearRegression()]\n",
    "}\n",
    "\n",
    "# create grid search\n",
    "grid = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'salary'\n",
    "# drop the na values in salary\n",
    "df_quality = df_quality.dropna(axis=0, subset=col_target)\n",
    "\n",
    "# drop cities column\n",
    "try:\n",
    "    df_quality = df_quality.drop('city', axis=1)\n",
    "except KeyError:  # if city column is already removed\n",
    "    pass\n",
    "# end try\n",
    "\n",
    "X = df_quality.drop(col_target, axis=1)\n",
    "y = df_quality.loc[:, col_target]\n",
    "\n",
    "# fit grid serach\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the best pipeline parameter combination and the score\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar plot for the mean test score over the number of categories\n",
    "mean_score = grid.cv_results_['mean_test_score']\n",
    "max_cat = grid.cv_results_[\n",
    "    'param_preprocessing__categorical_pipe__one_hot__max_categories'\n",
    "]\n",
    "tick_label = ['{}cats'.format(x) for x in max_cat]\n",
    "\n",
    "x = range(len(mean_score))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, mean_score)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(tick_label)\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "ax.set_ylim(bottom=.87, top=.89)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the final model once again with a fixed test and train set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=.3,\n",
    "    random_state=global_random_state\n",
    ")\n",
    "\n",
    "# build pipeline\n",
    "final_pipe = grid.best_estimator_\n",
    "\n",
    "# fit model\n",
    "final_pipe.fit(X_train, y_train)\n",
    "\n",
    "# print out train and test score\n",
    "train_score = final_pipe.score(X_train, y_train)\n",
    "test_score = final_pipe.score(X_test, y_test)\n",
    "print('Train score R2 is: {}'. format(\n",
    "    train_score\n",
    "))\n",
    "print('Test score R2 is: {}'. format(\n",
    "    test_score\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions on the train and test set\n",
    "y_train_pred = final_pipe.predict(X_train)\n",
    "y_test_pred = final_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot true vs. predicted value\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "\n",
    "def custom_error_plot(ax, plot_train, plot_residual):\n",
    "    \"\"\"\n",
    "    Function plots an arror plot in axis ax depending on the inputs:\n",
    "      - plot_train: bool, use train or test error\n",
    "      - plot_residual: bool, plot 'residual_vs_predicted' or\n",
    "        'actual_vs_predicted'\n",
    "    \"\"\"\n",
    "    if plot_train:\n",
    "        test_train_str = 'Train'\n",
    "        y = y_train\n",
    "        y_pred = y_train_pred\n",
    "\n",
    "    else:\n",
    "        test_train_str = 'Test'\n",
    "        y = y_test\n",
    "        y_pred = y_test_pred\n",
    "    # end if\n",
    "\n",
    "    if plot_residual:\n",
    "        kind_str = 'residual_vs_predicted'\n",
    "        title_kind_str = 'Residual vs Predicted'\n",
    "\n",
    "    else:\n",
    "        kind_str = 'actual_vs_predicted'\n",
    "        title_kind_str = 'Residual vs Predicted'\n",
    "    # end if\n",
    "\n",
    "    title_str = '{} set: {}'.format(\n",
    "        test_train_str,\n",
    "        title_kind_str\n",
    "    )\n",
    "\n",
    "    # set axis title\n",
    "    ax.set_title(title_str)\n",
    "\n",
    "    _ = PredictionErrorDisplay.from_predictions(\n",
    "        y,\n",
    "        y_pred,\n",
    "        kind=kind_str,\n",
    "        ax=ax,\n",
    "        subsample=None\n",
    "    )\n",
    "\n",
    "    return\n",
    "# end def\n",
    "\n",
    "\n",
    "# ### Figures with the train set ###\n",
    "# actual vs predicted\n",
    "custom_error_plot(axs[0][0], plot_train=True, plot_residual=False)\n",
    "\n",
    "# residual vs predicted\n",
    "custom_error_plot(axs[0][1], plot_train=True, plot_residual=True)\n",
    "\n",
    "# ### Figures with the test set ###\n",
    "# actual vs predicted\n",
    "custom_error_plot(axs[1][0], plot_train=False, plot_residual=False)\n",
    "\n",
    "# residual vs predicted\n",
    "custom_error_plot(axs[1][1], plot_train=False, plot_residual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importance and names\n",
    "final_model = final_pipe.named_steps['model']\n",
    "model_coefs = final_model.coef_\n",
    "importance = np.abs(model_coefs)\n",
    "feature_names = final_pipe.named_steps['preprocessing'].get_feature_names_out()\n",
    "\n",
    "# quick sanity check\n",
    "print('Number of features: {}'.format(len(feature_names)))\n",
    "print('Number of coefficients: {}'.format(len(model_coefs)))\n",
    "\n",
    "# sort feature names and importance by descending importance\n",
    "idx_sort = model_coefs.argsort()\n",
    "model_coefs_sorted = np.flip(model_coefs[idx_sort])\n",
    "feature_names_sorted = np.flip(feature_names[idx_sort])\n",
    "\n",
    "n_bars = 15  # plot only n_bars bars\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# common axes label names\n",
    "xlabel = 'feature names'\n",
    "ylabel = 'model coefficient value'\n",
    "\n",
    "# bar plot 1\n",
    "ax[0].bar(height=model_coefs_sorted[:n_bars], x=feature_names_sorted[:n_bars])\n",
    "ax[0].set_title((\"The {} most important positive features\\n\"\n",
    "                 \"by model coefficients\").format(n_bars))\n",
    "ax[0].set_xlabel(xlabel)\n",
    "ax[0].set_ylabel(ylabel)\n",
    "ax[0].tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# bar plot 2\n",
    "ax[1].bar(height=model_coefs_sorted[-n_bars:],\n",
    "          x=feature_names_sorted[-n_bars:])\n",
    "ax[1].set_title((\"The {} most important negative features\\n\"\n",
    "                 \"by model coefficients\").format(n_bars))\n",
    "ax[1].set_xlabel(xlabel)\n",
    "ax[1].set_ylabel(ylabel)\n",
    "ax[1].tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "# update and save figure\n",
    "plt.savefig(r'fig\\bar_features')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of countries or continents in the most important features.\n",
    "However some other factors seems to play an important role as well.\n",
    "\n",
    "E.g. public transport, taxi costs and the costs of milk.\n",
    "\n",
    "Below, I want to check whether the results are plausible in the model and the features are not messed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to test the milk component\n",
    "df_milk = df_quality.mode().iloc[0, :].to_frame().T\n",
    "df_milk = df_milk.convert_dtypes(convert_string=False)\n",
    "df_milk = pd.concat([df_milk]*2)\n",
    "df_milk = df_milk.set_index(pd.Index(['min', 'max']))\n",
    "df_milk.loc['min', 'milk'] = df_quality.loc[:, 'milk'].min()\n",
    "df_milk.loc['max', 'milk'] = df_quality.loc[:, 'milk'].max()\n",
    "\n",
    "df_milk = df_milk.loc[:, X_train.columns]\n",
    "print(df_milk.loc[:, 'milk'])\n",
    "final_pipe.predict(df_milk)\n",
    "# final_pipe.predict(X_test.iloc[0,:].to_frame().T.convert_dtypes(convert_string=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1eb35e6531bd04b4b837975573005e099e8ade0fd8b9ddc6a1acbd27b46e7bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
